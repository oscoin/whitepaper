\section{Osrank}
\label{s:osrank}

\def\Graph{\mathsf{Graph}}
\def\proj{\mathsf{project}}
\def\contributor{\mathsf{account}}
\def\dep{\mathsf{depend}}
\def\own{\mathsf{maintain}}
\def\coown{\mathsf{maintain}^\circ}
\def\contrib{\mathsf{contrib}}
\def\cocontrib{\mathsf{contrib}^\circ}

Since one of the fundamental mechanisms of \oscoin{} is to distribute a portion
of newly minted \oscoin{} ($B_r$) to high-value projects on the network, the protocol
needs to reach consensus on which projects are ``high value''. To choose which
projects receive \oscoin{}, and in which proportions, the network uses
\osrank{}, which is a modification of the well-know \pagerank{}
algorithm~\cite{pagerank} applied to the graph of relationships between
projects and contributions on the network, and in particular
\emph{dependencies} between projects. The output of the algorithm is a score
$\omega(P)$ associated to each project $P$.

\subsection{Intuition}

While the importance of an open source software package is highly subjective to
each individual, valuable information about its relative importance can be
extracted by studying the structure of open source software through
dependencies and contributions.

Similarly to how Google pioneered a large-scale search engine around PageRank
that took advantage of the structure in hypertext (the Web) in order
to organize the worldâ€™s information, the graph of open source software
dependencies and contributions offers an important resource when it comes to information
extraction, that can be thought of as an implicit form of trust
and relative value present in the ecosystem.

\subsection{Rationale}

While it may be clear that value is being produced by a system as a whole, it is
often unclear how much value particular subsystems are
contributing. Furthermore, subsystems have inherently differing capacities to
transform value into revenue: subsystems at the boundaries with other domains
are at an advantage, even though they may (transitively) derive much of their
value from other subsystems. This unbalance in value flows in and out of
subsystems is detrimental to the system as a whole.

Thankfully, human activity does not happen in a void\footnote{Space
  exploration will be addressed in a subsequent paper.}: be it
knowledge work, content/media creation, financial activity, etc.,
entities interact with one another in meaningful ways that are highly
influenced by the relative value each entity has with respect to the
system as a whole. Crucially, during this activity, a trail of
\emph{artefacts} (hypermedia links, contracts, transactions,
dependencies etc.) is produced; concrete manifestations
of the relationships between the various entities.

The basic insight of \pagerank{} and similar metrics, is that by
stochastic simulation of meaningful activity traces of system agents,
producing trails of artefacts consistent with observations, one can
learn about the value flows between subsystems, and hence approximate
the underlying value distribution. In the case of \pagerank{} one
simulates a human searching for high-relevance data over the Internet,
following a chain of links. A sophisticated algorithm would use as
many heuristics as possible for choosing the next link to follow (as a
real human would), \eg{} evaluating the relevance of the link-text. If
computational resources are scarce, the crudest algorithm simply picks
the next link at random, and this is what produces the classic
\pagerank{} formula which has seen so much success at ranking web
pages.

\subsection{Notation}

In this section we will use the following conventions: A \emph{graph} refers to
a finite directed graph. For such a graph $G$ the vertex set is denoted $V(G)$
and the edge-set is denoted $E(G) \subseteq V^2$. An edge $e = (x,y)$ of $G$ is
denoted $x \xrightarrow[]{e} y$. Given a subset $X$ of the vertices of $G$,
$G[X]$ denotes the \emph{induced subgraph} on $X$, that is, the graph with $X$
as the set of vertices and as edges the set
$\{ x \xrightarrow[]{e} y \mid e \in E(G), \{x,y\} \subseteq X\}$. A \emph{walk}
of length $n$ in a graph $G$ is a morphism of graphs $L_n \to G$ where $L_n$ is
the $n$-th linear-graph: $V(L_n) = \{i \in \mathbb{N} \mid i \leq n \}$,
$E(L_n) = \{ (i,i+1) \in \mathbb{N}^2 \mid i < n \}$.

\subsection{Ranking entities in \oscoin{}}
\label{s:netgraph}

The artefacts related to OSS development which are captured on the \oscoin{}
ledger are:
\begin{itemize}
  \item Maintenance of a project,
  \item Dependencies between projects,
  \item Signed code contributions to projects.
\end{itemize}
We summarize these interactions with a weighted directed graph $G$
(Figure~\ref{fig:G}), the weights corresponding to the relative importance of
the edge, which can be set as parameters to the algorithm.  The weights on all
edges outgoing from a vertex sum to $1$.

\begin{center}
\begin{tikzpicture}[vertex/.style={draw,circle,minimum width=8ex}, arr/.style={draw,thick,->}]
  \node[vertex] (p) at (2,0)  {\Small $\proj$};
  \node[vertex] (u) at (-2,0) {\Small $\contributor$};

  \draw[arr,looseness=7] (p) to[out=75,in=20] node[above] {\Small $\dep \ (\frac{4}{7})$} (p);
  \draw[arr] (p) to[out=165,in=15] node[above]   {\Small $\own \ (\frac{2}{7})$} (u);
  \draw[arr] (u) to[out=55,in=125] node[above]   {\Small $\coown \ (\frac{3}{5})$} (p);
  \draw[arr] (p) to[out=195,in=-15] node[below]  {\Small $\contrib \ (\frac{1}{7})$} (u);
  \draw[arr] (u) to[out=-55,in=-125] node[below] {\Small $\cocontrib \ (\frac{2}{5})$} (p);
\end{tikzpicture}
\captionof{figure}{\small Entity relations in open source software development with example weights.\label{fig:G}}
\end{center}
\medskip

% TODO: include simulations description and results here.

\noindent Note that some of the edges are bidirectional, but each direction gets
its own weight. For example both $\contrib$ and $\cocontrib$ pertain to
the same artefact: a contribution of code from an account to a
project. The edge $\contrib$ represents the statement \emph{the contributor
is valuable to the project}. The edge $\cocontrib$ going in the
opposite direction represents the statement \emph{the project is
valuable to the contributor}; the rationale for this less obvious flow is
that developers are unlikely to invest time contributing to a project they
don't find valuable.

% TODO: Make the above easier to grasp. Also maybe rephrase a little "is
% valueable to the project"

While the graph of dependencies between projects is the most straightforward
indication that a project derives value from another, adding edges corresponding
to contribution and maintenance serves two purposes:
\begin{itemize}
\item The graph is directional: it flows from user-facing applications to core
  libraries. This biases the score greatly towards basic libraries and
  development tools. Taking contributions into account brings in flows in the
  other direction, indicating which user facing applications developers
  value. For example, a full-fledged open source text editor is of high value
  to open source developers, yet it is never going to be a dependency of
  another piece of software. However, developers contributing to this text
  editor are likely to also contribute to packages the text editor depends
  on, and this allows the algorithm to flow back up from utility packages to
  user-facing packages.
\item The graph which results from adding contributor and maintainer accounts
  is a lot more \emph{connected}. The dependency graph alone is unlikely to
  exhibit much connections between vastly different domains, \eg{}
  web-programming and game-programming; web servers just aren't that
  interested in collision-detection. This can cause a niche of projects to
  form, of high value to the members of the open source community, but which
  doesn't have many currently highly-scored projects depending on them.
  Humans on the other hand often \emph{do} have vary varied interests,
  allowing these niches to connect back up to the rest of the ecosystem.
\end{itemize}
We propose to record these artefacts as transactions in a blockchain, so that
one may unambiguously rebuild and reach consensus on a finite directed graph
$\netgraph$ over $G$ (\ie{} in the slice category $\Graph/G$). We'll refer to
$\netgraph$ as the \emph{network graph}.

This in turn allows global value scores to be assigned to entities
according to a specific stochastic process simulating \emph{random
  walks}, that is, activity traces in $\netgraph$ with sampling
probabilities taken from $G$. For example: a contributor decides to
work on a high value project, contributes a changeset, in so doing adds a
dependency to another project, which imports a bug from upstream. The
contributor therefore switches to fixing said bug in the dependency, etc. Much
like a simulation of a human searching for information by clicking through
links, this simulated contributor activity informs us on the value flows
between subsystems in OSS development.
% TODO: I think the above example mixes too many value flows.

This score, which we call \osrank{}, is used to weight the
distribution of newly minted tokens to high value entities, thus
compensating entities which bring value to the ecosystem.

\subsection{Definition}

To implement \osrank{}, we use the following Monte Carlo-based algorithm: for
each vertex on the network, $R$ random walks are performed which start at that
vertex. To decide which next vertex to visit during a random walk, first a type
of outgoing edge is decided by sampling outgoing edges according to the weights
in $G$. The next choice is specific to the edge:

\begin{itemize}
\item In the cases of $\dep$ and $\own$, all vertices are equiprobable.
\item In the cases of edges $x \xrightarrow[]{\coown} p$ and
  $x \xrightarrow[]{\cocontrib} p$ outgoing from a contributor $x$, a project $p$ is
  chosen with probability $\frac{p_x}{N_x}$, where $p_x$ is the number of
  contributions $x$ has contributed to $p$, and $N_x$ is the total number of
  contributions by $x$.
\item In the case of edges $p \xrightarrow[]{\contrib} x$, the contributors $x$ are
  chosen with probability $\frac{p_x}{N}$, where $N$ is the total number of
  contributions $p$ has received from all contributors.
\end{itemize}

\noindent At each step the walk might terminate with probability $1 - \epsilon_\proj$ at a
project vertex, and $1 - \epsilon_\contributor$ at an account vertex, (here
$\epsilon_\proj, \epsilon_\contributor \in (0,1)$ are the \emph{damping factors}). This
produces a set $W$ of $nR$ random walks, where $n$ is the total number of
vertices. The \osrank{} for a project $x$ is then:
\[
  \omega(x) = \frac{W_x \epsilon_\proj}{n R}
\]
where $W_x$ is the number of times all the walks in $W$ visit $x$, that is:
\[
  W_x = \sum_{w \in W} |w[\{x\}]|.
\]
Similarly, the \osrank{} of an account $a$ is:
\[
  \omega(a) = \frac{W_a \epsilon_\contributor}{n R}.
\]

\subsection{Sybil networks}

The main problem to avoid is compensating illegitimate ecosystems, that is,
project structures and relationships which have been setup in the
network graph for the sole purpose of exploiting \osrank{}.
These attacks which are known as \emph{Sybil attacks} have been examined
before within the context of \pagerank{} \cite{pagerank-sybil}.

To address this we propose a modification of the \pagerank{} algorithm, based
on the ideas behind TrustRank~\cite{trustrank}, which involves two distinct
phases:

\begin{itemize}
\item In the first phase, all the random walks generated start in a specific
  seed set $\seedset$, a subset of all project and account vertices. Specifically, a random
  walk always starts at a vertex in $\seedset$. Otherwise the process remains
  the same. The scores obtained are biased towards interactions with the seed
  set, but they are not used directly by the compensation mechanism. Instead a
  threshold $t$ is used to determine the legitimacy of entities: any entity
  falling below this threshold is not considered for the next phase. The output
  of this phase is a subgraph $\netgraph_t$ of the network graph.
\item In the second phase, the algorithm is run on the subgraph
  $\netgraph_t$ as usual, with no seed set. The output of the second
  phase is the \osrank{}.
\end{itemize}
Care must be taken to ensure the seed set is large enough to touch most
legitimate projects on the network.  There are many ways to maintain such a
seed set. However, this will be discussed in future work.

\subsection{Implementation}

There are several options for making sure the computation of \osrank{}
will not be prohibitively expensive for network operators to run.
% TODO: Has "operators" been defined?

\begin{itemize}
\item \emph{Incremental Monte Carlo.} An incremental algorithm is used so that
  \osrank{} values may be updated as the network graph evolves. Instead of
  computing \osrank{} from scratch, we rely on the fact that only a small
  percentage of the vertices and edges of the network graph are added or removed
  from one calculation to the next (confirmed empirically on the datasets of
  major code-hosting platforms). Therefore most of the random walks that were
  performed in the previous calculation remain valid in the updated graph. For
  example if only a single dependency $p_1 \xrightarrow[]{\dep} p_2$ has been
  added since the last calculation, then a random walk that was generated on the
  old graph is only invalidated in the updated graph if it passes through $p_1$.

  In practice the operators of the network cache the set of random
  walks from the previous calculation, and update this set by removing
  invalid walks and replacing them with new ones. Details on such an
  incremental algorithm used in the case of \pagerank{} can be found
  in \cite{incr pagerank}.

  Since all the computations must be deterministic, the walks are not
  truly random. Rather, they must use a built-in pseudo-random number
  generator to generate the random walks. The seed for generating
  random walks is fixed in the genesis block.

  % TODO: Add details on RNG

\item \emph{Long epoch.} In this scenario payouts according to
  \osrank{} are only made infrequently, by choosing a large value for the
  reward epoch $\epoch$, \eg{} monthly.  In this case
  abusers might modify network structures of several projects in
  anticipation of the payout block. To mitigate this, edges are
  weighted by the amount of time they have existed in the last epoch. For
  example, if a dependency is added $n$ blocks before the reward block,
  then it only has a weight proportional to $n/\epoch$.
\end{itemize}

\subsection*{Conclusion}

Having \osrank{} available on the ledger is fundamental to the \oscoin{}
``monetary policy'', as explained in \S \ref{s:treasury}. Furthermore,
\osrank{} and $\netgraph$ as a whole will be available to smart contracts
(\S\ref{s:smart-contracts}) so that projects can script custom behaviors using
this data.
